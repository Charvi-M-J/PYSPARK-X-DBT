bronze_ingestion

df = spark.read.format("CSV")\
    .option("header", "true")\
    .option("inferSchema", "true")\
    .load("/Volumes/pysparkdbt/source/source_data/customers/")  

display(df)

df = spark.readStream.format("csv")\
    .option("header", "true")\
    .schema(schema_customers)\
    .load("/Volumes/pysparkdbt/source/source_data/customers/")  


df.writeStream.format("delta")\
    .outputMode("append")\
    .option("checkpointLocation","/Volumes/pysparkdbt/source/source_data/customers/")\
    .trigger(once=True)\
    .toTable("pysparkdbt.bronze.customers")   

schema_customers = df.schema
schema_customers

entities = ["customers","trips","locations","payments","vechicles","drivers"]

for entity in entities:   

     df_batch = spark.read.format("CSV")\
      .option("header", "true")\
      .option("inferSchema", "true")\
      .load(f"/Volumes/pysparkdbt/source/source_data/{entity}/")    

     schema_entity = df_batch.schema     
    
     df = spark.readStream.format("csv")\
            .option("header", "true")\
            .schema(schema_entity)\
            .load(f"/Volumes/pysparkdbt/source/source_data/{entity}/")  


     df.writeStream.format("delta")\
         .outputMode("append")\
         .option("checkpointLocation",f"/Volumes/pysparkdbt/bronze/checkpoints/{entity}/")\
         .trigger(once=True)\
         .toTable(f"pysparkdbt.bronze.{entity}")

CREATE VOLUME pysparkdbt.bronze.checkpoints;

